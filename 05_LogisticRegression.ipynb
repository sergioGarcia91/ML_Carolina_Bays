{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPOwhTywLtywOU1FO/zydQk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergioGarcia91/ML_Carolina_Bays/blob/main/05_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Notebook, 30 logistic regression models will be trained. Since category 0 has a higher number of pixels compared to category 1, **downsampling** will be applied to category 0.  \n",
        "\n",
        "The process will involve iteratively separating the data from both categories. In each iteration, the number of samples in category 1 will be counted, and the same number of samples from category 0 will be randomly selected to balance the dataset.  \n",
        "\n",
        "To increase randomness in training, a new **train-test split** will be performed in each iteration, ensuring that the training data for category 1 varies in every cycle.  \n"
      ],
      "metadata": {
        "id": "okRtjd_BpgyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start"
      ],
      "metadata": {
        "id": "KlKT_LU8Kl9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tables"
      ],
      "metadata": {
        "id": "9IUSI9q4iqa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import h5py\n",
        "import multiprocessing\n",
        "import joblib\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "hg1KD5WE1DMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KWQaix58tB1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "zJx7myk_LB3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_saveCSV = '/content/drive/MyDrive/UIS/Doctorado_UIS2198589/1_semestre/TopicosAvanzadosGeofisica/FC_CarolinaBais/Dataset_CSV'\n",
        "\n",
        "df = pd.read_hdf(os.path.join(path_saveCSV, 'TRAIN_CarolinaBays_AOI_01_03.h5'), 'df')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cgE60zCYLDSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total of data 103327744\n",
        "df.info()"
      ],
      "metadata": {
        "id": "4bf8tm7HLGga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split and training"
      ],
      "metadata": {
        "id": "_lrkux783gwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cores = multiprocessing.cpu_count()\n",
        "print(f\"Number of available cores: {num_cores}\")\n"
      ],
      "metadata": {
        "id": "Ldx8KhpnhGQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_save_models = '/content/drive/MyDrive/UIS/Doctorado_UIS2198589/1_semestre/TopicosAvanzadosGeofisica/FC_CarolinaBais/ML_models/'\n"
      ],
      "metadata": {
        "id": "CPcTeKP4L23T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_text = True\n",
        "print_text_Training = True\n",
        "verbose_print = True\n",
        "\n",
        "count_models = 31 # Indicate the model number that will be saved\n",
        "# If the process was stopped and you want to continue from the previous amount,\n",
        "# you should specify the number from which you want to start\n",
        "\n",
        "total_models = count_models + 10\n",
        "\n",
        "target_score = 0.6 # In the tests, it never exceeded a score of 0.6\n",
        "\n",
        "count_trial = 1\n",
        "\n",
        "train_score_list = []\n",
        "test_score_list = []\n",
        "models_name_list = []\n",
        "elapsed_time_list = []\n",
        "trial_list = []\n",
        "\n",
        "\n",
        "while count_models < total_models:\n",
        "  # Start the timer\n",
        "  start_time = time.time() # Each iteration takes less than 10 minutes\n",
        "\n",
        "  clear_output(wait=True)\n",
        "\n",
        "  # Create empty DataFrames for train and test\n",
        "  df_train = pd.DataFrame()\n",
        "  df_test = pd.DataFrame()\n",
        "\n",
        "  # Filter data\n",
        "  category_data_1 = df[df['y'] == 1].copy().reset_index(drop=True)\n",
        "  category_data_0 = df[df['y'] == 0].copy().reset_index(drop=True)\n",
        "\n",
        "  # Calculate 80% for train and 20% for test\n",
        "  train_size = int(0.8 * len(category_data_1))\n",
        "  test_size = len(category_data_1) - train_size\n",
        "\n",
        "  # Select randomly to shuffle the data\n",
        "  category_data_1 = category_data_1.sample(frac=1).reset_index(drop=True)\n",
        "  category_data_0 = category_data_0.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  # Split into train and test\n",
        "  category_train_1 = category_data_1[:train_size]\n",
        "  category_train_0 = category_data_0[:train_size]\n",
        "  category_test_1 = category_data_1[train_size:]\n",
        "  category_test_0 = category_data_0[train_size:]\n",
        "\n",
        "  category_train = pd.concat([category_train_1, category_train_0], ignore_index=True)\n",
        "  category_train = category_train.sample(frac=1).reset_index(drop=True)\n",
        "  category_test = pd.concat([category_test_1, category_test_0], ignore_index=True)\n",
        "  category_test = category_test.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  if print_text:\n",
        "    print(f'Train size: {len(category_train_1)*2}')\n",
        "    print(f'Test size: {len(category_test_1)*2}')\n",
        "    print('---'*3)\n",
        "\n",
        "  # Concatenate the data into the corresponding DataFrames\n",
        "  df_train = pd.concat([df_train, category_train], ignore_index=True)\n",
        "  df_test = pd.concat([df_test, category_test], ignore_index=True)\n",
        "  if print_text:\n",
        "    print(f'DF Train size: {df_train.shape[0]}')\n",
        "    print(f'DF Test size: {df_test.shape[0]}')\n",
        "    print('\\n')\n",
        "\n",
        "  # Datos to Train and Test\n",
        "  X_train = df_train.iloc[:, :-1].to_numpy()\n",
        "  y_train = df_train['y'].to_numpy()\n",
        "\n",
        "  X_test = df_test.iloc[:, :-1].to_numpy()\n",
        "  y_test = df_test['y'].to_numpy()\n",
        "\n",
        "  if print_text:\n",
        "    print('Shapes X_train, y_train, X_test, y_test')\n",
        "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "  # Create the model\n",
        "  model_LogReg = LogisticRegression(solver='saga',\n",
        "                                    verbose=verbose_print,\n",
        "                                    n_jobs=-1) # -1 means using all processors\n",
        "\n",
        "  # Train the model\n",
        "  print('---'*10)\n",
        "  print(f'Trial: {count_trial}')\n",
        "  model_LogReg.fit(X_train, y_train)\n",
        "\n",
        "  # End the timer\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  train_score = model_LogReg.score(X_train, y_train)\n",
        "  test_score = model_LogReg.score(X_test, y_test)\n",
        "\n",
        "  if train_score > target_score:\n",
        "    if print_text_Training:\n",
        "      print(f'Train score: {train_score:.4f}')\n",
        "      print(f'Test score: {test_score:.4f}')\n",
        "      print(f'Elapsed time: {elapsed_time:.2f} seconds')\n",
        "      print('\\n')\n",
        "\n",
        "    # Save model\n",
        "    if count_models < 10:\n",
        "      Name = f'model_RegLog_00{count_models}.pkl'\n",
        "    elif count_models < 100:\n",
        "      Name = f'model_RegLog_0{count_models}.pkl'\n",
        "    else:\n",
        "      Name = f'model_RegLog_{count_models}.pkl'\n",
        "\n",
        "    joblib.dump(model_LogReg, path_save_models + Name)\n",
        "    print(f'---> Model saved as {Name}')\n",
        "    print('\\n')\n",
        "\n",
        "    train_score_list.append(train_score)\n",
        "    test_score_list.append(test_score)\n",
        "    models_name_list.append(Name)\n",
        "    elapsed_time_list.append(round(elapsed_time, 2))\n",
        "    trial_list.append(count_trial)\n",
        "\n",
        "    count_models += 1\n",
        "\n",
        "  else:\n",
        "    print(f'Train score: {train_score:.4f}')\n",
        "    print(f'Elapsed time: {elapsed_time:.2f} seconds')\n",
        "    print('No model was generated.')\n",
        "    print('\\n')\n",
        "\n",
        "  count_trial += 1\n"
      ],
      "metadata": {
        "id": "1AwSk6Rq6Uqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Df models"
      ],
      "metadata": {
        "id": "O1cVZZOtSXJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_model = {'Trial': trial_list,\n",
        "              'Model': models_name_list,\n",
        "              'Train score': train_score_list,\n",
        "              'Test score': test_score_list,\n",
        "              'Elapsed time': elapsed_time_list} # Total time per iteration\n",
        "\n",
        "df_models = pd.DataFrame(dict_model)\n",
        "\n",
        "df_models"
      ],
      "metadata": {
        "id": "k4lDT1cESYxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_models.describe().round(2)"
      ],
      "metadata": {
        "id": "mzMk5QgDq11i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Df"
      ],
      "metadata": {
        "id": "zfde151lXQkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_models.to_csv(path_save_models + 'df_30models_LogReg.csv',\n",
        "                 sep=';',\n",
        "                 decimal=',',\n",
        "                 index=False)"
      ],
      "metadata": {
        "id": "ktEXzitIXSB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "J4IxO7FVKmLa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4adl-r1DKjbd"
      },
      "outputs": [],
      "source": []
    }
  ]
}